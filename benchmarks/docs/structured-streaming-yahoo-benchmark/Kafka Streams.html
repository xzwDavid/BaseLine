<!DOCTYPE html>
<html>
<head>
  <meta name="databricks-html-version" content="1">
<title>Kafka Streams - Databricks</title>

<meta charset="utf-8">
<meta name="google" content="notranslate">
<meta name="robots" content="nofollow">
<meta http-equiv="Content-Language" content="en">
<meta http-equiv="Content-Type" content="text/html; charset=UTF8">
<link rel="stylesheet"
  href="https://fonts.googleapis.com/css?family=Source+Code+Pro:400,700">

<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/lib/css/bootstrap.min.css">
<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/lib/jquery-ui-bundle/jquery-ui.min.css">
<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/css/main.css">
<link rel="stylesheet" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/css/print.css" media="print">
<link rel="icon" type="image/png" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/img/favicon.ico"/>
<script>window.settings = {"enableNotebookNotifications":true,"enableSshKeyUI":false,"defaultInteractivePricePerDBU":0.4,"enableClusterMetricsUI":true,"useReactTableCreateView":false,"enableOnDemandClusterType":true,"enableAutoCompleteAsYouType":[],"devTierName":"Community Edition","enableJobsPrefetching":true,"workspaceFeaturedLinks":[{"linkURI":"https://docs.databricks.com/index.html","displayName":"Documentation","icon":"question"},{"linkURI":"https://docs.databricks.com/release-notes/product/index.html","displayName":"Release Notes","icon":"code"},{"linkURI":"https://docs.databricks.com/spark/latest/training/index.html","displayName":"Training & Tutorials","icon":"graduation-cap"}],"enableClearStateFeature":true,"enableJobsAclsV2InUI":false,"dbcForumURL":"http://forums.databricks.com/","enableProtoClusterInfoDeltaPublisher":true,"enableAttachExistingCluster":true,"resetJobListOnConnect":true,"serverlessDefaultSparkVersion":"latest-stable-scala2.11","maxCustomTags":45,"serverlessDefaultMaxWorkers":20,"enableInstanceProfilesUIInJobs":true,"nodeInfo":{"node_types":[{"support_ssh":false,"spark_heap_memory":4800,"instance_type_id":"r3.2xlarge","spark_core_oversubscription_factor":8.0,"node_type_id":"dev-tier-node","description":"Community Optimized","support_cluster_tags":false,"container_memory_mb":6000,"node_instance_type":{"instance_type_id":"r3.2xlarge","provider":"AWS","local_disk_size_gb":160,"compute_units":26.0,"number_of_ips":14,"local_disks":1,"reserved_compute_units":3.64,"gpus":0,"memory_mb":62464,"num_cores":8,"local_disk_type":"AHCI","max_attachable_disks":0,"supported_disk_types":[{"ebs_volume_type":"GENERAL_PURPOSE_SSD"},{"ebs_volume_type":"THROUGHPUT_OPTIMIZED_HDD"}],"reserved_memory_mb":4800},"memory_mb":6144,"is_hidden":false,"category":"Community Edition","num_cores":0.88,"support_port_forwarding":false,"support_ebs_volumes":false,"is_deprecated":false}],"default_node_type_id":"dev-tier-node"},"sqlAclsDisabledMap":{"spark.databricks.acl.enabled":"false","spark.databricks.acl.sqlOnly":"false"},"enableDatabaseSupportClusterChoice":true,"enableClusterAcls":true,"notebookRevisionVisibilityHorizon":999999,"serverlessClusterProductName":"Serverless Pool","showS3TableImportOption":true,"maxEbsVolumesPerInstance":10,"isAdmin":true,"deltaProcessingBatchSize":1000,"timerUpdateQueueLength":100,"sqlAclsEnabledMap":{"spark.databricks.acl.enabled":"true","spark.databricks.acl.sqlOnly":"true"},"enableLargeResultDownload":true,"maxElasticDiskCapacityGB":5000,"serverlessDefaultMinWorkers":2,"zoneInfos":[{"id":"us-west-2c","isDefault":true},{"id":"us-west-2b","isDefault":false},{"id":"us-west-2a","isDefault":false}],"enableCustomSpotPricingUIByTier":false,"serverlessClustersEnabled":false,"enableFindAndReplace":true,"enableEBSVolumesUIForJobs":true,"enablePublishNotebooks":true,"enableBitbucketCloud":true,"enableMaxConcurrentRuns":true,"enableJobAclsConfig":false,"enableFullTextSearch":false,"enableElasticSparkUI":false,"enableNewClustersCreate":true,"clusters":true,"allowRunOnPendingClusters":true,"useAutoscalingByDefault":false,"enableAzureToolbar":false,"fileStoreBase":"FileStore","enableEmailInAzure":false,"enableRLibraries":true,"enableSshKeyUIInJobs":true,"enableDetachAndAttachSubMenu":true,"configurableSparkOptionsSpec":[{"keyPattern":"spark\\.kryo(\\.[^\\.]+)+","valuePattern":".*","keyPatternDisplay":"spark.kryo.*","valuePatternDisplay":"*","description":"Configuration options for Kryo serialization"},{"keyPattern":"spark\\.io\\.compression\\.codec","valuePattern":"(lzf|snappy|org\\.apache\\.spark\\.io\\.LZFCompressionCodec|org\\.apache\\.spark\\.io\\.SnappyCompressionCodec)","keyPatternDisplay":"spark.io.compression.codec","valuePatternDisplay":"snappy|lzf","description":"The codec used to compress internal data such as RDD partitions, broadcast variables and shuffle outputs."},{"keyPattern":"spark\\.serializer","valuePattern":"(org\\.apache\\.spark\\.serializer\\.JavaSerializer|org\\.apache\\.spark\\.serializer\\.KryoSerializer)","keyPatternDisplay":"spark.serializer","valuePatternDisplay":"org.apache.spark.serializer.JavaSerializer|org.apache.spark.serializer.KryoSerializer","description":"Class to use for serializing objects that will be sent over the network or need to be cached in serialized form."},{"keyPattern":"spark\\.rdd\\.compress","valuePattern":"(true|false)","keyPatternDisplay":"spark.rdd.compress","valuePatternDisplay":"true|false","description":"Whether to compress serialized RDD partitions (e.g. for StorageLevel.MEMORY_ONLY_SER). Can save substantial space at the cost of some extra CPU time."},{"keyPattern":"spark\\.speculation","valuePattern":"(true|false)","keyPatternDisplay":"spark.speculation","valuePatternDisplay":"true|false","description":"Whether to use speculation (recommended off for streaming)"},{"keyPattern":"spark\\.es(\\.[^\\.]+)+","valuePattern":".*","keyPatternDisplay":"spark.es.*","valuePatternDisplay":"*","description":"Configuration options for ElasticSearch"},{"keyPattern":"es(\\.([^\\.]+))+","valuePattern":".*","keyPatternDisplay":"es.*","valuePatternDisplay":"*","description":"Configuration options for ElasticSearch"},{"keyPattern":"spark\\.(storage|shuffle)\\.memoryFraction","valuePattern":"0?\\.0*([1-9])([0-9])*","keyPatternDisplay":"spark.(storage|shuffle).memoryFraction","valuePatternDisplay":"(0.0,1.0)","description":"Fraction of Java heap to use for Spark's shuffle or storage"},{"keyPattern":"spark\\.streaming\\.backpressure\\.enabled","valuePattern":"(true|false)","keyPatternDisplay":"spark.streaming.backpressure.enabled","valuePatternDisplay":"true|false","description":"Enables or disables Spark Streaming's internal backpressure mechanism (since 1.5). This enables the Spark Streaming to control the receiving rate based on the current batch scheduling delays and processing times so that the system receives only as fast as the system can process. Internally, this dynamically sets the maximum receiving rate of receivers. This rate is upper bounded by the values `spark.streaming.receiver.maxRate` and `spark.streaming.kafka.maxRatePerPartition` if they are set."},{"keyPattern":"spark\\.streaming\\.receiver\\.maxRate","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.receiver.maxRate","valuePatternDisplay":"numeric","description":"Maximum rate (number of records per second) at which each receiver will receive data. Effectively, each stream will consume at most this number of records per second. Setting this configuration to 0 or a negative number will put no limit on the rate. See the deployment guide in the Spark Streaming programing guide for mode details."},{"keyPattern":"spark\\.streaming\\.kafka\\.maxRatePerPartition","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.kafka.maxRatePerPartition","valuePatternDisplay":"numeric","description":"Maximum rate (number of records per second) at which data will be read from each Kafka partition when using the Kafka direct stream API introduced in Spark 1.3. See the Kafka Integration guide for more details."},{"keyPattern":"spark\\.streaming\\.kafka\\.maxRetries","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.kafka.maxRetries","valuePatternDisplay":"numeric","description":"Maximum number of consecutive retries the driver will make in order to find the latest offsets on the leader of each partition (a default value of 1 means that the driver will make a maximum of 2 attempts). Only applies to the Kafka direct stream API introduced in Spark 1.3."},{"keyPattern":"spark\\.streaming\\.ui\\.retainedBatches","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.ui.retainedBatches","valuePatternDisplay":"numeric","description":"How many batches the Spark Streaming UI and status APIs remember before garbage collecting."}],"enableReactNotebookComments":true,"enableAdminPasswordReset":false,"checkBeforeAddingAadUser":false,"enableResetPassword":true,"maxClusterTagValueLength":255,"enableJobsSparkUpgrade":true,"perClusterAutoterminationEnabled":false,"enableNotebookCommandNumbers":true,"sparkVersions":[{"key":"1.6.3-db2-hadoop2-scala2.10","displayName":"Spark 1.6.3-db2 (Hadoop 2, Scala 2.10)","packageLabel":"spark-image-aba860a0ffce4f3471fb14aefdcb1d768ac66a53a5ad884c48745ef98aeb9d67","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"3.3.x-gpu-scala2.11","displayName":"3.3 (includes Apache Spark 2.2.0, GPU, Scala 2.11)","packageLabel":"spark-image-4ccf711c90fd8d633b5629f18c840ae0a337b85583bb20438c73de5a94f19099","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.1.1-db5-scala2.11","displayName":"Spark 2.1.1-db5 (Scala 2.11)","packageLabel":"spark-image-08d9fc1551087e0876236f19640c4a83116b1649f15137427d21c9056656e80e","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"1.6.x-ubuntu15.10","displayName":"Spark 1.6.x (Hadoop 1)","packageLabel":"spark-image-8cea23fb9094e174bf5815d79009f4a8e383eb86cf2909cf6c6434ed8da2a16a","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"3.3.x-scala2.10","displayName":"3.3 (includes Apache Spark 2.2.0, Scala 2.10)","packageLabel":"spark-image-88f10af7324d00498d3cb8f6d808d32d5c60474845a5c264096b409ade4f25fb","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.4.x-ubuntu15.10","displayName":"Spark 1.4.1 (Hadoop 1, deprecated)","packageLabel":"spark-image-f710650fb8aaade8e4e812368ea87c45cd8cd0b5e6894ca6c94f3354e8daa6dc","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"2.2.x-scala2.11","displayName":"3.0 (includes Apache Spark 2.2.0, Scala 2.11)","packageLabel":"spark-image-67ab3a06d1e83d5b60df7063245eb419a2e9fe329aeeb7e7d9713332c669bb17","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"2.1.1-db6-scala2.10","displayName":"Spark 2.1.1-db6 (Scala 2.10)","packageLabel":"spark-image-177f3f02a6a3432d30068332dc857b9161345bdd2ee8a2d2de05bb05cb4b0f4c","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.1.0-db2-scala2.11","displayName":"Spark 2.1.0-db2 (Scala 2.11)","packageLabel":"spark-image-267c4490a3ab8a39acdbbd9f1d36f6decdecebf013e30dd677faff50f1d9cf8b","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"2.1.x-gpu-scala2.11","displayName":"Spark 2.1 (Auto-updating, GPU, Scala 2.11 experimental)","packageLabel":"spark-image-d613235f93e0f29838beb2079a958c02a192ed67a502192bc67a8a5f2fb37f35","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.0-ubuntu15.10-scala2.10","displayName":"Spark 2.0.0 (Scala 2.10)","packageLabel":"spark-image-073c1b52ace74f251fae2680624a0d8d184a8b57096d1c21c5ce56c29be6a37a","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"latest-stable-gpu-scala2.11","displayName":"Latest stable (3.3, GPU, Scala 2.11)","packageLabel":"spark-image-4ccf711c90fd8d633b5629f18c840ae0a337b85583bb20438c73de5a94f19099","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"2.0.2-db3-scala2.10","displayName":"Spark 2.0.2-db3 (Scala 2.10)","packageLabel":"spark-image-584091dedb690de20e8cf22d9e02fdcce1281edda99eedb441a418d50e28088f","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"3.2.x-scala2.10","displayName":"3.2 (includes Apache Spark 2.2.0, Scala 2.10)","packageLabel":"spark-image-3ef6d6cc156adc3024eaff9e47af444e30a5cc9e61d09c34e8ae9e1b0a4e4960","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"latest-experimental-scala2.10","displayName":"Latest experimental (3.4 snapshot, Scala 2.10)","packageLabel":"spark-image-bb5421d9cdabc7dc2935236438d0327851f90b2ccf58f4b4e055e920808934cc","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"2.1.0-db1-scala2.11","displayName":"Spark 2.1.0-db1 (Scala 2.11)","packageLabel":"spark-image-e8ad5b72cf0f899dcf2b4720c1f572ab0e87a311d6113b943b4e1d4a7edb77eb","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"2.1.1-db4-scala2.11","displayName":"Spark 2.1.1-db4 (Scala 2.11)","packageLabel":"spark-image-52bca0ca866e3f4243d3820a783abf3b9b3b553edf234abef14b892657ceaca9","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"latest-rc-scala2.11","displayName":"Latest RC (3.4, Scala 2.11)","packageLabel":"spark-image-4714221efbf9c26b1aec95cdc46b495fc4f85c4d051d26909794593f61e17ce6","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"latest-stable-scala2.11","displayName":"Latest stable (3.3, Scala 2.11)","packageLabel":"spark-image-d9361b34430b88997c4e8d9bb3e2003e77cfbd7af75948968fdb30a40338cd43","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"2.1.0-db2-scala2.10","displayName":"Spark 2.1.0-db2 (Scala 2.10)","packageLabel":"spark-image-a2ca4f6b58c95f78dca91b1340305ab3fe32673bd894da2fa8e1dc8a9f8d0478","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"1.6.x-ubuntu15.10-hadoop1","displayName":"Spark 1.6.x (Hadoop 1)","packageLabel":"spark-image-8cea23fb9094e174bf5815d79009f4a8e383eb86cf2909cf6c6434ed8da2a16a","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"2.0.2-db4-scala2.11","displayName":"Spark 2.0.2-db4 (Scala 2.11)","packageLabel":"spark-image-7dbc7583e8271765b8a1508cb9e832768e35489bbde2c4c790bc6766aee2fd7f","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.1-ubuntu15.10-hadoop1","displayName":"Spark 1.6.1 (Hadoop 1)","packageLabel":"spark-image-21d1cac181b7b8856dd1b4214a3a734f95b5289089349db9d9c926cb87d843db","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"2.0.x-gpu-scala2.11","displayName":"Spark 2.0 (Auto-updating, GPU, Scala 2.11 experimental)","packageLabel":"spark-image-968b89f1d0ec32e1ee4dacd04838cae25ef44370a441224177a37980d539d83a","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.2-ubuntu15.10-hadoop1","displayName":"Spark 1.6.2 (Hadoop 1)","packageLabel":"spark-image-8cea23fb9094e174bf5815d79009f4a8e383eb86cf2909cf6c6434ed8da2a16a","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"1.6.3-db1-hadoop2-scala2.10","displayName":"Spark 1.6.3-db1 (Hadoop 2, Scala 2.10)","packageLabel":"spark-image-eaa8d9b990015a14e032fb2e2e15be0b8d5af9627cd01d855df728b67969d5d9","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"1.6.3-db2-hadoop1-scala2.10","displayName":"Spark 1.6.3-db2 (Hadoop 1, Scala 2.10)","packageLabel":"spark-image-14112ea0645bea94333a571a150819ce85573cf5541167d905b7e6588645cf3b","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.2-ubuntu15.10-hadoop2","displayName":"Spark 1.6.2 (Hadoop 2)","packageLabel":"spark-image-161245e66d887cd775e23286a54bab0b146143e1289f25bd1732beac454a1561","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"1.6.1-ubuntu15.10-hadoop2","displayName":"Spark 1.6.1 (Hadoop 2)","packageLabel":"spark-image-4cafdf8bc6cba8edad12f441e3b3f0a8ea27da35c896bc8290e16b41fd15496a","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"2.0.2-db2-scala2.10","displayName":"Spark 2.0.2-db2 (Scala 2.10)","packageLabel":"spark-image-36d48f22cca7a907538e07df71847dd22aaf84a852c2eeea2dcefe24c681602f","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"2.0.x-ubuntu15.10-scala2.11","displayName":"Spark 2.0 (Ubuntu 15.10, Scala 2.11, deprecated)","packageLabel":"spark-image-8e1c50d626a52eac5a6c8129e09ae206ba9890f4523775f77af4ad6d99a64c44","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"2.0.x-scala2.10","displayName":"Spark 2.0 (Auto-updating, Scala 2.10)","packageLabel":"spark-image-859e88079f97f58d50e25163b39a1943d1eeac0b6939c5a65faba986477e311a","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.1.1-db4-scala2.10","displayName":"Spark 2.1.1-db4 (Scala 2.10)","packageLabel":"spark-image-c7c0224de396cd1563addc1ae4bca6ba823780b6babe6c3729ddf73008f29ba4","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"latest-rc-scala2.10","displayName":"Latest RC (3.4, Scala 2.10)","packageLabel":"spark-image-bb5421d9cdabc7dc2935236438d0327851f90b2ccf58f4b4e055e920808934cc","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"latest-stable-scala2.10","displayName":"Latest stable (3.3, Scala 2.10)","packageLabel":"spark-image-88f10af7324d00498d3cb8f6d808d32d5c60474845a5c264096b409ade4f25fb","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"2.0.2-db1-scala2.11","displayName":"Spark 2.0.2-db1 (Scala 2.11)","packageLabel":"spark-image-c2d623f03dd44097493c01aa54a941fc31978ebe6d759b36c75b716b2ff6ab9c","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"2.0.2-db4-scala2.10","displayName":"Spark 2.0.2-db4 (Scala 2.10)","packageLabel":"spark-image-859e88079f97f58d50e25163b39a1943d1eeac0b6939c5a65faba986477e311a","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.1.1-db5-scala2.10","displayName":"Spark 2.1.1-db5 (Scala 2.10)","packageLabel":"spark-image-74133df2c13950431298d1cab3e865c191d83ac33648a8590495c52fc644c654","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"1.5.x-ubuntu15.10","displayName":"Spark 1.5.2 (Hadoop 1, deprecated)","packageLabel":"spark-image-c9d2a8abf41f157a4acc6d52bc721090346f6fea2de356f3a66e388f54481698","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"latest-experimental-gpu-scala2.11","displayName":"Latest experimental (3.4 snapshot, GPU, Scala 2.11)","packageLabel":"spark-image-209b840fdb1f9317fd85a79f375d7ab94030b26e6b025a8644963e6fc4f68ca4","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"2.2.x-scala2.10","displayName":"3.0 (includes Apache Spark 2.2.0, Scala 2.10)","packageLabel":"spark-image-d549f2d4a523994ecdf37e531b51d5ec7d8be51534bb0ca5322eaad28ba8f557","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"3.0.x-scala2.11","displayName":"3.0 (includes Apache Spark 2.2.0, Scala 2.11)","packageLabel":"spark-image-67ab3a06d1e83d5b60df7063245eb419a2e9fe329aeeb7e7d9713332c669bb17","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"2.0.x-scala2.11","displayName":"Spark 2.0 (Auto-updating, Scala 2.11)","packageLabel":"spark-image-7dbc7583e8271765b8a1508cb9e832768e35489bbde2c4c790bc6766aee2fd7f","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.1.x-scala2.10","displayName":"Spark 2.1 (Auto-updating, Scala 2.10)","packageLabel":"spark-image-177f3f02a6a3432d30068332dc857b9161345bdd2ee8a2d2de05bb05cb4b0f4c","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"3.1.x-scala2.11","displayName":"3.1 (includes Apache Spark 2.2.0, Scala 2.11)","packageLabel":"spark-image-0e50da7a73d3bc76fcca8ac8a67c1e036c5a30deb5663adf6d0f332cc8ec2c90","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"2.1.0-db3-scala2.10","displayName":"Spark 2.1.0-db3 (Scala 2.10)","packageLabel":"spark-image-25a17d070af155f10c4232dcc6248e36a2eb48c24f8d4fc00f34041b86bd1626","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"2.0.2-db2-scala2.11","displayName":"Spark 2.0.2-db2 (Scala 2.11)","packageLabel":"spark-image-4fa852ba378e97815083b96c9cada7b962a513ec23554a5fc849f7f1dd8c065a","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"3.1.x-scala2.10","displayName":"3.1 (includes Apache Spark 2.2.0, Scala 2.10)","packageLabel":"spark-image-0c03d5f78139022c37032b5f3ffac5b5a4445d9cab8733e333f16a67a47262f9","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"3.3.x-scala2.11","displayName":"3.3 (includes Apache Spark 2.2.0, Scala 2.11)","packageLabel":"spark-image-d9361b34430b88997c4e8d9bb3e2003e77cfbd7af75948968fdb30a40338cd43","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.3.x-ubuntu15.10","displayName":"Spark 1.3.0 (Hadoop 1, deprecated)","packageLabel":"spark-image-40d2842670bc3dc178b14042501847d76171437ccf70613fa397a7a24c48b912","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"2.0.1-db1-scala2.11","displayName":"Spark 2.0.1-db1 (Scala 2.11)","packageLabel":"spark-image-10ab19f634bbfdb860446c326a9f76dc25bfa87de6403b980566279142a289ea","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"2.0.2-db3-scala2.11","displayName":"Spark 2.0.2-db3 (Scala 2.11)","packageLabel":"spark-image-7fd7aaa89d55692e429115ae7eac3b1a1dc4de705d50510995f34306b39c2397","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"2.1.1-db6-scala2.11","displayName":"Spark 2.1.1-db6 (Scala 2.11)","packageLabel":"spark-image-fdad9ef557700d7a8b6bde86feccbcc3c71d1acdc838b0fd299bd19956b1076e","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.3-db1-hadoop1-scala2.10","displayName":"Spark 1.6.3-db1 (Hadoop 1, Scala 2.10)","packageLabel":"spark-image-d50af1032799546b8ccbeeb76889a20c819ebc2a0e68ea20920cb30d3895d3ae","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"2.0.2-db1-scala2.10","displayName":"Spark 2.0.2-db1 (Scala 2.10)","packageLabel":"spark-image-654bdd6e9bad70079491987d853b4b7abf3b736fff099701501acaabe0e75c41","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"2.0.x-ubuntu15.10","displayName":"Spark 2.0 (Ubuntu 15.10, Scala 2.10, deprecated)","packageLabel":"spark-image-a659f3909d51b38d297b20532fc807ecf708cfb7440ce9b090c406ab0c1e4b7e","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"latest-experimental-scala2.11","displayName":"Latest experimental (3.4 snapshot, Scala 2.11)","packageLabel":"spark-image-4714221efbf9c26b1aec95cdc46b495fc4f85c4d051d26909794593f61e17ce6","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"3.2.x-scala2.11","displayName":"3.2 (includes Apache Spark 2.2.0, Scala 2.11)","packageLabel":"spark-image-aca37a44b05c9c0177ca8a639ad186ebace4aac1a7bdcf73829945ab47414f41","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.1-db1-scala2.10","displayName":"Spark 2.0.1-db1 (Scala 2.10)","packageLabel":"spark-image-5a13c2db3091986a4e7363006cc185c5b1108c7761ef5d0218506cf2e6643840","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"2.1.x-scala2.11","displayName":"Spark 2.1 (Auto-updating, Scala 2.11)","packageLabel":"spark-image-fdad9ef557700d7a8b6bde86feccbcc3c71d1acdc838b0fd299bd19956b1076e","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.1.0-db1-scala2.10","displayName":"Spark 2.1.0-db1 (Scala 2.10)","packageLabel":"spark-image-f0ab82a5deb7908e0d159e9af066ba05fb56e1edb35bdad41b7ad2fd62a9b546","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"3.0.x-scala2.10","displayName":"3.0 (includes Apache Spark 2.2.0, Scala 2.10)","packageLabel":"spark-image-d549f2d4a523994ecdf37e531b51d5ec7d8be51534bb0ca5322eaad28ba8f557","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"1.6.0-ubuntu15.10","displayName":"Spark 1.6.0 (Hadoop 1)","packageLabel":"spark-image-10ef758029b8c7e19cd7f4fb52fff9180d75db92ca071bd94c47f3c1171a7cb5","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"1.6.x-ubuntu15.10-hadoop2","displayName":"Spark 1.6.x (Hadoop 2)","packageLabel":"spark-image-161245e66d887cd775e23286a54bab0b146143e1289f25bd1732beac454a1561","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"2.0.0-ubuntu15.10-scala2.11","displayName":"Spark 2.0.0 (Scala 2.11)","packageLabel":"spark-image-b4ec141e751f201399f8358a82efee202560f7ed05e1a04a2ae8778f6324b909","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"2.1.0-db3-scala2.11","displayName":"Spark 2.1.0-db3 (Scala 2.11)","packageLabel":"spark-image-ccbc6b73f158e2001fc1fb8c827bfdde425d8bd6d65cb7b3269784c28bb72c16","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"latest-rc-gpu-scala2.11","displayName":"Latest RC (3.4 GPU, Scala 2.11)","packageLabel":"spark-image-209b840fdb1f9317fd85a79f375d7ab94030b26e6b025a8644963e6fc4f68ca4","upgradable":true,"deprecated":false,"customerVisible":false}],"enablePresentationMode":false,"enableClearStateAndRunAll":true,"enableRestrictedClusterCreation":true,"enableFeedback":true,"enableClusterAutoScaling":false,"enableUserVisibleDefaultTags":true,"defaultNumWorkers":0,"serverContinuationTimeoutMillis":10000,"jobsUnreachableThresholdMillis":60000,"driverStderrFilePrefix":"stderr","enableNotebookRefresh":false,"accountsOwnerUrl":"https://accounts.cloud.databricks.com/registration.html#login","driverStdoutFilePrefix":"stdout","showDbuPricing":true,"databricksDocsBaseHostname":"docs.databricks.com","defaultNodeTypeToPricingUnitsMap":{"r3.2xlarge":2,"i3.4xlarge":4,"class-node":1,"m4.2xlarge":1.5,"r4.xlarge":1,"m4.4xlarge":3,"r4.16xlarge":16,"Standard_DS11":0.5,"p2.8xlarge":16,"m4.10xlarge":8,"r3.8xlarge":8,"r4.4xlarge":4,"dev-tier-node":1,"c3.8xlarge":4,"r3.4xlarge":4,"i2.4xlarge":6,"m4.xlarge":0.75,"r4.8xlarge":8,"r4.large":0.5,"Standard_DS12":1,"development-node":1,"i2.2xlarge":3,"g2.8xlarge":6,"i3.large":0.75,"memory-optimized":1,"m4.large":0.375,"p2.16xlarge":24,"i3.8xlarge":8,"i3.16xlarge":16,"Standard_DS12_v2":1,"Standard_DS13":2,"Standard_DS11_v2":0.5,"Standard_DS13_v2":2,"c3.2xlarge":1,"Standard_L4s":1.5,"c4.2xlarge":1,"i2.xlarge":1.5,"compute-optimized":1,"c4.4xlarge":2,"i3.2xlarge":2,"c3.4xlarge":2,"g2.2xlarge":1.5,"p2.xlarge":2,"m4.16xlarge":12,"c4.8xlarge":4,"i3.xlarge":1,"r3.xlarge":1,"r4.2xlarge":2,"i2.8xlarge":12},"enableSparkDocsSearch":true,"sparkHistoryServerEnabled":true,"enableEBSVolumesUI":false,"metastoreServiceRowLimit":1000000,"enableIPythonImportExport":true,"enableClusterTagsUIForJobs":true,"enableClusterTagsUI":false,"enableNotebookHistoryDiffing":true,"branch":"2.55","accountsLimit":3,"enableSparkEnvironmentVariables":true,"enableX509Authentication":false,"useAADLogin":false,"enableStructuredStreamingNbOptimizations":true,"enableNotebookGitBranching":true,"local":false,"enableNotebookLazyRenderWrapper":false,"enableClusterAutoScalingForJobs":false,"enableStrongPassword":false,"showReleaseNote":true,"displayDefaultContainerMemoryGB":6,"enableNotebookCommandMode":true,"disableS3TableImport":false,"deploymentMode":"production","useSpotForWorkers":true,"removePasswordInAccountSettings":false,"preferStartTerminatedCluster":false,"enableUserInviteWorkflow":true,"enableStaticNotebooks":true,"sandboxForUrlSandboxFrame":"allow-scripts allow-popups allow-popups-to-escape-sandbox allow-forms","enableCssTransitions":true,"serverlessEnableElasticDisk":true,"minClusterTagKeyLength":1,"showHomepageFeaturedLinks":true,"pricingURL":"https://databricks.com/product/pricing","enableClusterAclsConfig":false,"useTempS3UrlForTableUpload":false,"notifyLastLogin":false,"enableSshKeyUIByTier":false,"enableCreateClusterOnAttach":true,"defaultAutomatedPricePerDBU":0.2,"enableNotebookGitVersioning":true,"defaultMinWorkers":2,"files":"files/","feedbackEmail":"feedback@databricks.com","enableDriverLogsUI":true,"defaultMaxWorkers":8,"enableWorkspaceAclsConfig":false,"dropzoneMaxFileSize":2047,"enableNewClustersList":true,"enableNewDashboardViews":true,"driverLog4jFilePrefix":"log4j","enableSingleSignOn":true,"enableMavenLibraries":true,"displayRowLimit":1000,"deltaProcessingAsyncEnabled":true,"enableSparkEnvironmentVariablesUI":false,"defaultSparkVersion":{"key":"3.2.x-scala2.11","displayName":"3.2 (includes Apache Spark 2.2.0, Scala 2.11)","packageLabel":"spark-image-aca37a44b05c9c0177ca8a639ad186ebace4aac1a7bdcf73829945ab47414f41","upgradable":true,"deprecated":false,"customerVisible":true},"enableCustomSpotPricing":false,"enableMountAclsConfig":false,"defaultAutoterminationMin":180,"useDevTierHomePage":true,"enableClusterClone":true,"enableNotebookLineNumbers":true,"enablePublishHub":false,"notebookHubUrl":"http://hub.dev.databricks.com/","showSqlEndpoints":false,"enableNotebookDatasetInfoView":true,"enableClusterAclsByTier":false,"databricksDocsBaseUrl":"https://docs.databricks.com/","azurePortalLink":"https://portal.azure.com","cloud":"AWS","disallowAddingAdmins":true,"enableSparkConfUI":true,"featureTier":"DEVELOPER_BASIC_TIER","mavenCentralSearchEndpoint":"http://search.maven.org/solrsearch/select","enableOrgSwitcherUI":true,"bitbucketCloudBaseApiV2Url":"https://api.bitbucket.org/2.0","clustersLimit":1,"enableJdbcImport":true,"enableElasticDisk":false,"logfiles":"logfiles/","enableRelativeNotebookLinks":true,"enableMultiSelect":true,"enableWebappSharding":true,"enableClusterDeltaUpdates":true,"enableSingleSignOnLogin":false,"separateTableForJobClusters":true,"ebsVolumeSizeLimitGB":{"GENERAL_PURPOSE_SSD":[100,4096],"THROUGHPUT_OPTIMIZED_HDD":[500,4096]},"enableMountAcls":false,"requireEmailUserName":true,"dbcFeedbackURL":"mailto:feedback@databricks.com","enableMountAclService":true,"enableStructuredDataAcls":false,"showVersion":true,"serverlessClustersByDefault":false,"enableWorkspaceAcls":false,"maxClusterTagKeyLength":127,"gitHash":"7e487e8906bd141e3032c363a823d3c48d1535b5","showWorkspaceFeaturedLinks":true,"signupUrl":"https://databricks.com/try-databricks","serverlessAttachEbsVolumesByDefault":false,"enableTokensConfig":false,"allowFeedbackForumAccess":true,"enableImportFromUrl":true,"enableTokens":false,"enableMiniClusters":true,"enableNewJobList":true,"enableDebugUI":false,"enableStreamingMetricsDashboard":true,"allowNonAdminUsers":true,"enableSingleSignOnByTier":false,"enableJobsRetryOnTimeout":true,"useStandardTierUpgradeTooltips":true,"staticNotebookResourceUrl":"https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/","enableSpotClusterType":true,"enableSparkPackages":true,"checkAadUserInWorkspaceTenant":false,"dynamicSparkVersions":true,"enableClusterTagsUIByTier":false,"enableNotebookHistoryUI":true,"enableClusterLoggingUI":true,"enableDatabaseDropdownInTableUI":true,"showDebugCounters":false,"enableInstanceProfilesUI":false,"enableFolderHtmlExport":true,"homepageFeaturedLinks":[{"linkURI":"https://docs.databricks.com/_static/notebooks/gentle-introduction-to-apache-spark.html","displayName":"Introduction to Apache Spark on Databricks","icon":"img/home/Python_icon.svg"},{"linkURI":"https://docs.databricks.com/_static/notebooks/databricks-for-data-scientists.html","displayName":"Databricks for Data Scientists","icon":"img/home/Scala_icon.svg"},{"linkURI":"https://docs.databricks.com/_static/notebooks/structured-streaming-python.html","displayName":"Introduction to Structured Streaming","icon":"img/home/Python_icon.svg"}],"enableClusterStart":false,"enableEBSVolumesUIByTier":false,"singleSignOnComingSoon":false,"removeSubCommandCodeWhenExport":true,"upgradeURL":"https://accounts.cloud.databricks.com/registration.html#login","maxAutoterminationMinutes":10000,"autoterminateClustersByDefault":true,"notebookLoadingBackground":"#fff","sshContainerForwardedPort":2200,"enableServerAutoComplete":true,"enableStaticHtmlImport":true,"enableInstanceProfilesByTier":false,"showForgotPasswordLink":true,"defaultMemoryPerContainerMB":6000,"enablePresenceUI":true,"minAutoterminationMinutes":10,"accounts":true,"useOnDemandClustersByDefault":true,"useFramedStaticNotebooks":false,"enableNewProgressReportUI":true,"enableAutoCreateUserUI":true,"defaultCoresPerContainer":4,"showTerminationReason":true,"enableNewClustersGet":true,"showPricePerDBU":false,"showSqlProxyUI":true,"enableNotebookErrorHighlighting":true};</script>
<script>var __DATABRICKS_NOTEBOOK_MODEL = {"version":"NotebookV1","origId":2704664478301946,"name":"Kafka Streams","language":"scala","commands":[{"version":"CommandV1","origId":2704664478301948,"guid":"d18c98c8-2354-4ee1-8ae0-a989e29e6989","subtype":"command","commandType":"auto","position":0.125,"command":"%md # Apache Kafka - Kafka Streams\n\nMethods and declarations required for running the Yahoo Benchmark with Kafka Streams.","commandVersion":0,"state":"error","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"6348789a-5206-4ae7-819f-2b104cc548f4"},{"version":"CommandV1","origId":2704664478301949,"guid":"585496d1-ddd4-4214-b09e-42314f3d1b4a","subtype":"command","commandType":"auto","position":0.25,"command":"%run ./Utils","commandVersion":0,"state":"error","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":1498012745053,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"d51d35b0-9ed4-483d-b699-88ed5e443499"},{"version":"CommandV1","origId":2704664478301952,"guid":"9098dd7b-a631-4273-a223-854efc430861","subtype":"command","commandType":"auto","position":0.5,"command":"package com.databricks.benchmark.kafka\n\nimport org.apache.kafka.clients.consumer.ConsumerRecord\nimport org.apache.kafka.streams.processor.TimestampExtractor\n\nimport com.databricks.benchmark.yahoo.Event\n\n// Extracts the embedded timestamp of a record (giving you \"event-time\" semantics).\nclass MyEventTimeExtractor extends TimestampExtractor {\n\n  override def extract(record: ConsumerRecord[Object, Object], previousTimestamp: Long): Long = record.value match {\n    case e: Event => e.event_time.getTime\n    case _ => record.timestamp\n  }\n}\n\nobject KStreamsUtils extends Serializable {\n  import org.apache.kafka.streams.StreamsConfig\n  import org.apache.kafka.common.serialization.Serdes\n  import org.apache.kafka.clients.consumer.ConsumerConfig\n  import org.apache.kafka.clients.producer.KafkaProducer\n  import java.util.Properties\n  \n  /** Default properties for our Kafka Streams application. */\n  def getStreamProps(kafkaNodesString: String, numThreads: Int = 1, appId: String): Properties = {\n    val props = new Properties();\n    props.put(StreamsConfig.APPLICATION_ID_CONFIG, appId)\n    props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, kafkaNodesString)\n    props.put(StreamsConfig.NUM_STREAM_THREADS_CONFIG, numThreads.toString)\n    props.put(StreamsConfig.KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass().getName())\n    props.put(StreamsConfig.VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass().getName())\n    props.put(StreamsConfig.TIMESTAMP_EXTRACTOR_CLASS_CONFIG, classOf[MyEventTimeExtractor].getName())\n\n    props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"earliest\")\n    props\n  }\n  \n  /** Creates a Kafka Producer. */\n  def getProducer(kafkaNodesString: String): KafkaProducer[String, String] = {\n    val producer2Props = new Properties()\n    producer2Props.put(\"bootstrap.servers\", kafkaNodesString)\n    producer2Props.put(\"key.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\")\n    producer2Props.put(\"value.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\")\n    new KafkaProducer[String, String](producer2Props)\n  }\n}","commandVersion":0,"state":"error","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":1498012773328,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"6518411f-f8ba-4311-bb40-a1bdb8514891"},{"version":"CommandV1","origId":2704664478301955,"guid":"d164bcff-3f35-41fc-8e09-7ee9386788ea","subtype":"command","commandType":"auto","position":0.75,"command":"package com.databricks.benchmark.kafka.serializers\n\nimport java.sql.Timestamp\n\nimport scala.reflect.ClassTag\n\nimport org.apache.kafka.common.serialization.{Deserializer, Serializer}\n\nimport com.databricks.benchmark.utils.Json\nimport com.databricks.benchmark.yahoo._\n\n/** Serdes for our classes */\n\nclass JsonSerializer[T] extends Serializer[T] {\n\n  override def configure(configs: java.util.Map[String, _], isKey: Boolean): Unit = {}\n  override def serialize(topic: String, data: T): Array[Byte] = {\n    Json.getBytes(data)\n  }\n  \n  override def close(): Unit = {}\n}\n\nclass JsonDeserializer[T](clazz: Class[T]) extends Deserializer[T] {\n  \n  implicit val recordCmt: ClassTag[T] = ClassTag(clazz)\n  override def configure(configs: java.util.Map[String, _], isKey: Boolean): Unit = {}\n  override def deserialize(topic: String, data: Array[Byte]): T = {\n    if (data == null) return null.asInstanceOf[T]\n    try {\n      Json.parseBytes(data)\n    } catch {\n      case scala.util.control.NonFatal(e) =>\n        println(s\"Error during ${clazz.getName} deserialization\", e)\n        null.asInstanceOf[T]\n    }\n  }\n  override def close(): Unit = {}\n}\n\n\ncase class CampaignAndTimestamp(campaign_id: String, timestamp: Timestamp) {\n  def this() = this(null, null)\n}\n\n","commandVersion":0,"state":"error","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":1498013247811,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"81fffe8f-7408-4576-bc4e-7e554ceeba39"},{"version":"CommandV1","origId":2704664478301958,"guid":"81f4cc43-0f81-4739-b049-dfd28b5c26e3","subtype":"command","commandType":"auto","position":0.875,"command":"package com.databricks.benchmark.kafka\n\nimport java.sql.Timestamp\nimport java.util.Properties\n\nimport org.apache.kafka.streams.StreamsConfig\nimport org.apache.kafka.common.serialization.Serdes\nimport org.apache.kafka.clients.consumer.ConsumerConfig\nimport org.apache.kafka.clients.producer.{KafkaProducer, ProducerRecord}\nimport org.apache.kafka.streams._\nimport org.apache.kafka.streams.kstream._\nimport org.apache.kafka.streams.processor._\n\nimport org.apache.spark.TaskContext\n\nimport com.databricks.benchmark.kafka.serializers._\nimport com.databricks.benchmark.kafka.KStreamsUtils\nimport com.databricks.benchmark.yahoo._\nimport com.databricks.benchmark.utils.Json\n\ncase class WindowCounter(var count: Long, var lastUpdate: Timestamp) {\n  def this() = this(0L, new Timestamp(0L))\n}\n\nobject YahooBenchmarkUtils {\n  val MEASUREMENTS_TOPIC = \"measurements\"\n  \n  /**\n   * The code for KafkaStreams that replicates the YahooBenchmark.\n   */\n  def runInExecutor(\n      kafkaNodesString: String,\n      numThreads: Int,\n      kafkaProducer: KafkaProducer[String, String]): KafkaStreams = {\n    val builder = new KStreamBuilder()\n    val kEvents: KStream[String, Event] = builder.stream(\n      Serdes.String(),\n      Serdes.serdeFrom(new JsonSerializer[Event], new JsonDeserializer(classOf[Event])),\n      Variables.EVENTS_TOPIC)\n    \n    /** We get the campaigns table as a KTable. */\n    val kCampaigns: KTable[String, CampaignAd] = builder.table(\n      Serdes.String(),\n      Serdes.serdeFrom(new JsonSerializer[CampaignAd], new JsonDeserializer(classOf[CampaignAd])),\n      Variables.CAMPAIGNS_TOPIC,\n      \"campaign-state\")\n    \n    kCampaigns.print()\n    \n    // Won't be exact since the multi-threaded nature of streams, but should be a good enough approximate\n    var cnt = 0\n    \n    /** This logs the number of records processed by one deployment of the Kafka Streams application to Kafka. */\n    TaskContext.get().addTaskCompletionListener { _ => \n      kafkaProducer.send(new ProducerRecord(MEASUREMENTS_TOPIC, cnt.toString))\n      kafkaProducer.close()\n    }\n    \n    val filteredEvents = kEvents.mapValues[Event](new ValueMapper[Event, Event] { \n      override def apply(value: Event): Event = {\n        cnt += 1\n        value\n      }\n    }).filter(new Predicate[String, Event] {\n      override def test(key: String, value: Event): Boolean = {\n        value.event_type == \"view\" \n      }\n    }).mapValues[ProjectedEvent](new ValueMapper[Event, ProjectedEvent] { \n      override def apply(value: Event): ProjectedEvent = {\n        ProjectedEvent(value.ad_id, value.event_time)\n      }\n    })\n\n    val joined: KStream[String, CampaignAndTimestamp] = filteredEvents.join(kCampaigns,\n      new ValueJoiner[ProjectedEvent, CampaignAd, CampaignAndTimestamp] {\n        override def apply(value1: ProjectedEvent, value2: CampaignAd): CampaignAndTimestamp = {\n          CampaignAndTimestamp(value2.campaign_id, value1.event_time)\n        }\n      }, Serdes.String(), Serdes.serdeFrom(new JsonSerializer[ProjectedEvent], new JsonDeserializer(classOf[ProjectedEvent])))\n\n    val keyedByCampaign: KStream[String, CampaignAndTimestamp] = joined.selectKey[String](\n      new KeyValueMapper[String, CampaignAndTimestamp, String] { \n        override def apply(key: String, value: CampaignAndTimestamp): String = {\n          value.campaign_id\n        }\n      })\n    \n    val initializer = new Initializer[WindowCounter] { override def apply(): WindowCounter = new WindowCounter() }\n    val aggregator = new Aggregator[String, CampaignAndTimestamp, WindowCounter] {\n      override def apply(key: String, value: CampaignAndTimestamp, aggregate: WindowCounter): WindowCounter = {\n        aggregate.count += 1\n        aggregate.lastUpdate = if (aggregate.lastUpdate.getTime > value.timestamp.getTime) aggregate.lastUpdate else value.timestamp\n        aggregate\n      }\n    }\n\n    val counts = keyedByCampaign.groupByKey(\n        Serdes.String(),\n        Serdes.serdeFrom(new JsonSerializer[CampaignAndTimestamp], new JsonDeserializer(classOf[CampaignAndTimestamp])))\n      .aggregate(\n        initializer,\n        aggregator,\n        TimeWindows.of(10000),\n        Serdes.serdeFrom(new JsonSerializer[WindowCounter], new JsonDeserializer(classOf[WindowCounter])),\n        \"time-windows\")\n\n    val output: KStream[String, Output] = counts.toStream().map[String, Output](\n      new KeyValueMapper[Windowed[String], WindowCounter, KeyValue[String, Output]] { \n        override def apply(key: Windowed[String], value: WindowCounter) = {\n          KeyValue.pair(key.key(), Output(new java.sql.Timestamp(key.window().start()), key.key(), value.count, value.lastUpdate))\n        }\n      })\n\n    output.to(Serdes.String(), Serdes.serdeFrom(new JsonSerializer[Output], new JsonDeserializer(classOf[Output])), Variables.OUTPUT_TOPIC)\n\n    new KafkaStreams(builder, KStreamsUtils.getStreamProps(kafkaNodesString, numThreads, \"yahoo-benchmark\"))\n  }\n}","commandVersion":0,"state":"error","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":1498013389919,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"2063356d-e228-4a68-954d-ae5a2370e9a6"},{"version":"CommandV1","origId":2704664478301959,"guid":"bdd86c13-7e83-48d8-a06e-11ff02e93ca4","subtype":"command","commandType":"auto","position":0.9375,"command":"import com.databricks.benchmark.kafka.{KStreamsUtils, YahooBenchmarkUtils}\nimport com.databricks.benchmark.yahoo._\n\n/**\n * Runner for the Yahoo Benchmark using Kafka Streams. Prefer setting `numExecutors` to configure parallelism rather than\n * `numThreadsPerExecutor` to have throughput calculations be more accurate.\n */\nclass KStreamsYahooRunner(\n    override val spark: SparkSession,\n    numExecutors: Int,\n    numThreadsPerExecutor: Int,\n    kafkaCluster: LocalKafka) extends YahooBenchmarkRunner {\n  import org.apache.kafka.clients.producer.ProducerRecord\n  \n  require(numExecutors >= 1, \"numExecutors can't be less than 1\")\n  require(numThreadsPerExecutor >= 1, \"numThreadsPerExecutor can't be less than 1\")\n  \n  override def start(): Unit = {\n    kafkaCluster.deleteTopicIfExists(YahooBenchmarkUtils.MEASUREMENTS_TOPIC)\n    kafkaCluster.createTopic(YahooBenchmarkUtils.MEASUREMENTS_TOPIC, partitions = 1, replFactor = 1)\n    sc.setJobGroup(\"kstreamsYahoo\", \"Kafka Streams Consumer\", true)\n    val kafkaNodes = kafkaCluster.kafkaNodesString\n    try {\n      // We launch KafkaStreams applications as long running Spark jobs\n      val numThreadsPerExecutor = this.numThreadsPerExecutor\n      sc.parallelize(Seq.empty[String], numExecutors).foreachPartition { _ =>\n        val producer = KStreamsUtils.getProducer(kafkaNodes)\n        val kafkaStreams = YahooBenchmarkUtils.runInExecutor(kafkaNodes, numThreadsPerExecutor, producer)\n        try {\n          kafkaStreams.start()\n          producer.send(new ProducerRecord[String, String](YahooBenchmarkUtils.MEASUREMENTS_TOPIC, \"0\"))\n          while (true) {\n            Thread.sleep(1000000)\n          }\n        } finally {\n          kafkaStreams.close()\n        }\n      }\n    } catch {\n      case scala.util.control.NonFatal(e) => println(e)\n    }\n  }\n  \n  override def stop(): Unit = {\n    super.stop()\n    sc.cancelJobGroup(\"kstreamsYahoo\")\n  }\n  \n  override val params: Map[String, Any] = Map(\"numThreads\" -> numExecutors * numThreadsPerExecutor)\n  \n  import org.apache.spark.sql.{DataFrame, Encoder}\n  import org.apache.spark.sql.expressions.Window\n  import org.apache.spark.sql.functions._\n  \n  /**\n   * For Kafka Streams, we write the number of records processed at the beginning of the benchmark (0) and the end of the benchmark\n   * to a topic in Kafka. We use the Kafka ingestion timestamps for these records to calculate the duration of the benchmark and\n   * we sum the number of records written to Kafka to get the total number of records processed.\n   */\n  override def getThroughput(): DataFrame = {\n    val realTimeMs = udf((t: java.sql.Timestamp) => t.getTime)\n\n    spark.read.format(\"kafka\")\n      .option(\"kafka.bootstrap.servers\", kafkaCluster.kafkaNodesString)\n      .option(\"subscribe\", YahooBenchmarkUtils.MEASUREMENTS_TOPIC)\n      .load()\n      .select('value.cast(\"string\"), 'timestamp)\n      .select(min('timestamp) as 'start, max('timestamp) as 'end, sum('value) as 'totalInput)\n      .withColumn(\"totalDurationMillis\", realTimeMs('end) - realTimeMs('start))\n      .withColumn(\"throughput\", 'totalInput * 1000 / 'totalDurationMillis)\n  }\n  \n  /**\n   * The latency calculation in Kafka Streams is a bit tricky due to data being generated outside the system, and we didn't want to\n   * add extra overhead of running an arbitrary aggregator (if there is one) rather than doing a simple count. The calculation works\n   * as follows:\n   *  1. We replay the benchmark using Spark in batch mode. We order events by their Kafka ingest timestamps and figure out what the\n   *     timestamp of the latest records would have to be to get a certain `count` for a given `campaign_id` and `time_window`.\n   *  2. We then join the latest record timestamps calculated in (1) with the output of Kafka Streams.\n   *  3. Latency is then the Kafka ingest timestamp of output in (2) - the latest record timestamp in (1).\n   */\n  override def getLatency(): DataFrame = {\n    import org.apache.spark.sql.types._\n    val schema = YahooBenchmark.outputSchema.add(\"lastUpdate\", LongType)\n    val realTimeMs = udf((t: java.sql.Timestamp) => t.getTime)\n    \n    spark.read\n      .format(\"kafka\")\n      .option(\"kafka.bootstrap.servers\", kafkaCluster.kafkaNodesString)\n      .option(\"subscribe\", Variables.OUTPUT_TOPIC)\n      .load()\n      .withColumn(\"result\", from_json($\"value\".cast(\"string\"), schema))\n      .select(\n        $\"timestamp\" as 'resultOutput,\n        $\"result.*\")\n      .groupBy($\"time_window\", $\"campaign_id\")\n      .agg(max($\"resultOutput\") as 'resultOutput, max('lastUpdate) as 'lastUpdate)\n      .withColumn(\"diff\", realTimeMs($\"resultOutput\") - 'lastUpdate)\n      .selectExpr(\n        \"min(diff) as latency_min\",\n        \"mean(diff) as latency_avg\",\n        \"percentile_approx(diff, 0.95) as latency_95\",\n        \"percentile_approx(diff, 0.99) as latency_99\",\n        \"max(diff) as latency_max\")\n  }\n}","commandVersion":0,"state":"error","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":1497470650674,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"ebbfb429-1d86-4c43-af27-6208ff6be739"}],"dashboards":[],"guid":"00963411-bc51-4779-ba0c-a76a346dc790","globalVars":{},"iPythonMetadata":null,"inputWidgets":{}};</script>
<script
 src="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/js/notebook-main.js"
 onerror="window.mainJsLoadError = true;"></script>
<script>var tableOfContentsCell = {"version":"CommandV1","origId":0,"guid":"83012cbd-6013-4380-b2bf-07134fdb2e6b","subtype":"command","commandType":"auto","position":0.0,"command":"%md [&lsaquo; Back to Table of Contents](index.html)","commandVersion":1,"state":"finished","results":{"type":"raw","data":"","arguments":{}},"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{}};</script>
</head>
<body>
  <script>
if (window.mainJsLoadError) {
  var u = 'https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/js/notebook-main.js';
  var b = document.getElementsByTagName('body')[0];
  var c = document.createElement('div');
  c.innerHTML = ('<h1>Network Error</h1>' +
    '<p><b>Please check your network connection and try again.</b></p>' +
    '<p>Could not load a required resource: ' + u + '</p>');
  c.style.margin = '30px';
  c.style.padding = '20px 50px';
  c.style.backgroundColor = '#f5f5f5';
  c.style.borderRadius = '5px';
  b.appendChild(c);
}
</script>
</body>
</html>